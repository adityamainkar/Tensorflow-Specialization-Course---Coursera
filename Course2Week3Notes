from tensorflow.keras.applications.inception_v3 import InceptionV3

local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'

pre_trained_model = InceptionV3(input_shape = (150, 150, 3), 
                                include_top = False, 
                                weights = None)

pre_trained_model.load_weights(local_weights_file)

for layer in pre_trained_model.layers:
  layer.trainable = False
last_layer = pre_trained_model.get_layer('mixed7')
print('last layer output shape: ', last_layer.output_shape)
last_output = last_layer.output
x = layers.Flatten()(last_output)
# Add a fully connected layer with 1,024 hidden units and ReLU activation
x = layers.Dense(1024, activation='relu')(x)
# Add a dropout rate of 0.2
x = layers.Dropout(0.2)(x)                  
# Add a final sigmoid layer for classification
x = layers.Dense  (1, activation='sigmoid')(x)           

model = Model( pre_trained_model.input, x) 

model.compile(optimizer = RMSprop(lr=0.0001), 
              loss = 'binary_crossentropy', 
              metrics = ['acc'])
              
              
              
How do you change the number of classes the model can classify when using transfer learning?
(i.e. the original model handled 1000 classes, but yours handles just 2)
When you add your DNN at the bottom of the network, you specify your output layer with the number of classes you want

When dropout = 0.2:  how many nodes will I lose?
All 20% (Not just untrained 20%)

Why do dropouts help avoid overfitting?

Because neighbor neurons can have similar weights, and thus can skew the final training

What would the symptom of a Dropout rate being set too high?
The network would lose specialization to the effect that it would be inefficient or ineffective at learning, driving accuracy down
